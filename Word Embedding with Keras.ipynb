{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4001a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "s1 = 'Taste is bad.'\n",
    "s2 = 'Steak is chewy and bad.'\n",
    "s3 = 'So bad and sad hate the steak'\n",
    "\n",
    "sentences = [s1,s2,s3]\n",
    "\n",
    "#initialize tokenizer\n",
    "tokenizer = Tokenizer() \n",
    "#if there's a lot of text in a doc/sentence Tokenizer(num_words = n), where n is limit number of words to form a vector\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded5ff0",
   "metadata": {},
   "source": [
    "# เปลี่ยนประโยค เป็น Vectors (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8c840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see all sentences/docs in the tokenizer\n",
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75058153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'bad': 3,\n",
       "             'is': 2,\n",
       "             'taste': 1,\n",
       "             'steak': 2,\n",
       "             'chewy': 1,\n",
       "             'and': 2,\n",
       "             'so': 1,\n",
       "             'the': 1,\n",
       "             'hate': 1,\n",
       "             'sad': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each word appears in [how many] docs/sentences\n",
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2631aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('taste', 1),\n",
       "             ('is', 2),\n",
       "             ('bad', 3),\n",
       "             ('steak', 2),\n",
       "             ('chewy', 1),\n",
       "             ('and', 2),\n",
       "             ('so', 1),\n",
       "             ('sad', 1),\n",
       "             ('hate', 1),\n",
       "             ('the', 1)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of words in all docs\n",
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c882a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad': 1,\n",
       " 'is': 2,\n",
       " 'steak': 3,\n",
       " 'and': 4,\n",
       " 'taste': 5,\n",
       " 'chewy': 6,\n",
       " 'so': 7,\n",
       " 'sad': 8,\n",
       " 'hate': 9,\n",
       " 'the': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word index as dict (this will be used for analysis)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba784e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc42d9d",
   "metadata": {},
   "source": [
    "### Text to index (Encoding)\n",
    "ก่อนใช้เป็น input ต้อง encode ก่อน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39456719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 2, 1], [3, 2, 6, 4, 1], [7, 1, 4, 8, 9, 10, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode the sentence\n",
    "sent_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "sent_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737165f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steak is chewy and bad.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, 2, 6, 4, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare encoded sentance with real sentence\n",
    "print(s2)\n",
    "tokenizer.texts_to_sequences([s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd89c3a",
   "metadata": {},
   "source": [
    "### Padding\n",
    "ปกติแล้วเวลาจะ Input ข้อมูลเข้าไป ต้องมที input_shape เท่าๆกัน ดังนั้นต้อง padding เพื่อ ทำให้ input_shape เท่ากัน\n",
    "แปลว่า Padding คือ ขั้นตอนทำ data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f87a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the sentences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7aca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "#let number of words in a longest sentence\n",
    "max_len = max([len(i) for i in sent_encoded])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b44b451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  2  1  0  0  0  0]\n",
      " [ 3  2  6  4  1  0  0]\n",
      " [ 7  1  4  8  9 10  3]]\n"
     ]
    }
   ],
   "source": [
    "#start padding (encoded_sentences, trucating='post' or 'pre' ตัดออก, padding='post' or 'pre' เติม0ที่ไหน, maxlen=max_len)\n",
    "sents_pad = pad_sequences(sent_encoded, truncating='post', padding='post', maxlen=max_len)\n",
    "print(sents_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4294714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taste is bad', 'steak is chewy and bad', 'so bad and sad hate the steak']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(sents_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d458a",
   "metadata": {},
   "source": [
    "# เปลี่ยนคำเป็น Vectors (Word Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b8c9716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 #เนื่องจากการประมวลใส่คำไม่มี 0\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbce297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 7, 5)              55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 15:43:38.389920: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-22 15:43:38.390543: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#define desired vector length\n",
    "embed_len = 5\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embed_len, input_length=max_len)\n",
    "])\n",
    "\n",
    "#vocab_size ที่ต้อง + 1 เพราะตอนแรก word_index มันไม่ได้เริ่มจาก 0 แต่ทีนี้เรา padding ไป ก็เลยมี 0 ที่ไม่มีความหมายอะไร\n",
    "#embed_len คือ size ของ vector คำที่ต้องการ ยิ่งค่าเยอะ ยิ่งละเอียดมาก เช่นของ GOOGLE Word2Vec มี size = 300\n",
    "#input_length คือความยาวของประโยค/เอกสารนั้นๆ หลังจากที่ Process แล้ว (Padded or Trunicated)\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary() #ค่า Param = 55 มาจาก vocab_size x embed_len = 11 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20dee5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 15:43:38.555570: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-22 15:43:38.600059: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 7, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = model.predict(sents_pad)\n",
    "vectors.shape #(number of docs, number of words, embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb54b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.009 -0.038 -0.047 -0.021 -0.014]\n",
      "  [-0.039 -0.009 -0.013 -0.04   0.02 ]\n",
      "  [-0.013  0.002 -0.039 -0.047  0.021]\n",
      "  [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "  [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "  [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "  [ 0.032 -0.036  0.043 -0.003 -0.008]]\n",
      "\n",
      " [[-0.014 -0.001  0.04  -0.013 -0.012]\n",
      "  [-0.039 -0.009 -0.013 -0.04   0.02 ]\n",
      "  [-0.046  0.045 -0.043  0.015  0.026]\n",
      "  [-0.023 -0.029  0.045  0.027 -0.03 ]\n",
      "  [-0.013  0.002 -0.039 -0.047  0.021]\n",
      "  [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "  [ 0.032 -0.036  0.043 -0.003 -0.008]]\n",
      "\n",
      " [[ 0.032 -0.026 -0.011  0.026 -0.036]\n",
      "  [-0.013  0.002 -0.039 -0.047  0.021]\n",
      "  [-0.023 -0.029  0.045  0.027 -0.03 ]\n",
      "  [ 0.003  0.011  0.046 -0.005 -0.048]\n",
      "  [-0.012  0.036 -0.006  0.019  0.001]\n",
      "  [-0.027  0.027  0.041 -0.016 -0.045]\n",
      "  [-0.014 -0.001  0.04  -0.013 -0.012]]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e85bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.009 -0.038 -0.047 -0.021 -0.014]\n"
     ]
    }
   ],
   "source": [
    "#vector คำว่า taste\n",
    "print(vectors[0][0].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35fe74e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence, Word, Vector\n",
      "---------------------------------------------------------\n",
      "      1    taste [-0.009 -0.038 -0.047 -0.021 -0.014]\n",
      "      1       is [-0.039 -0.009 -0.013 -0.04   0.02 ]\n",
      "      1      bad [-0.013  0.002 -0.039 -0.047  0.021]\n",
      "      1          [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "      1          [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "      1          [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "      1          [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "      2    steak [-0.014 -0.001  0.04  -0.013 -0.012]\n",
      "      2       is [-0.039 -0.009 -0.013 -0.04   0.02 ]\n",
      "      2    chewy [-0.046  0.045 -0.043  0.015  0.026]\n",
      "      2      and [-0.023 -0.029  0.045  0.027 -0.03 ]\n",
      "      2      bad [-0.013  0.002 -0.039 -0.047  0.021]\n",
      "      2          [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "      2          [ 0.032 -0.036  0.043 -0.003 -0.008]\n",
      "      3       so [ 0.032 -0.026 -0.011  0.026 -0.036]\n",
      "      3      bad [-0.013  0.002 -0.039 -0.047  0.021]\n",
      "      3      and [-0.023 -0.029  0.045  0.027 -0.03 ]\n",
      "      3      sad [ 0.003  0.011  0.046 -0.005 -0.048]\n",
      "      3     hate [-0.012  0.036 -0.006  0.019  0.001]\n",
      "      3      the [-0.027  0.027  0.041 -0.016 -0.045]\n",
      "      3    steak [-0.014 -0.001  0.04  -0.013 -0.012]\n"
     ]
    }
   ],
   "source": [
    "#ดูค่า vector แทน word\n",
    "print('Sentence, Word, Vector')\n",
    "print('---------------------------------------------------------')\n",
    "for i, sents in enumerate(vectors):\n",
    "    for j, word_v in enumerate(sents):\n",
    "        words = tokenizer.sequences_to_texts(sents_pad)[i].split() #จะได้เป็น List ของ คำทั้งหมดใน 1 ประโยค\n",
    "        if j < len(words):\n",
    "            print(f'{i+1:7} {words[j]:>8} {word_v.round(3)}')\n",
    "        else:\n",
    "            print(f'{i+1:7} {\"\":>8} {word_v.round(3)}') #blank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow M1",
   "language": "python",
   "name": "tf_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
